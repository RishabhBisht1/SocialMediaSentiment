{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c3ed747-57c3-4a26-a9ba-2d363e63395e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in c:\\users\\rishabh bisht\\appdata\\roaming\\python\\python312\\site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rishabh bisht\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\rishabh bisht\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\rishabh bisht\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\rishabh bisht\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\rishabh bisht\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\rishabh bisht\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\rishabh bisht\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.33.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\rishabh bisht\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\programdata\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.9.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rishabh bisht\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rishabh bisht\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rishabh bisht\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rishabh bisht\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rishabh bisht\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\rishabh bisht\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rishabh bisht\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rishabh bisht\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rishabh bisht\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rishabh bisht\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.9.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rishabh bisht\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22f15a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds1 = load_dataset(\"rahmaabusalma/tweets_sentiment_analysis\")\n",
    "ds2 = load_dataset(\"AmaanP314/youtube-comment-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "512eea26-e60f-4688-9b61-8160d67d1b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: emoji in c:\\users\\rishabh bisht\\appdata\\roaming\\python\\python312\\site-packages (2.14.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87d14ef0-ca26-4f32-a46d-6161d14624a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'text', 'label', 'sentiment'],\n",
      "        num_rows: 31232\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'text', 'label', 'sentiment'],\n",
      "        num_rows: 5205\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'text', 'label', 'sentiment'],\n",
      "        num_rows: 5206\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0187fdb8-7ed3-44f6-913a-bbae021f02d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['CommentID', 'VideoID', 'VideoTitle', 'AuthorName', 'AuthorChannelID', 'CommentText', 'Sentiment', 'Likes', 'Replies', 'PublishedAt', 'CountryCode', 'CategoryID'],\n",
      "        num_rows: 1032225\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05452022-4f7b-4463-bbb7-608076838a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Anyone know what movie this is? Neutral\n",
      "1 The fact they're holding each other back while equally being most aggressive 😂😂 Positive\n",
      "2 waiting next video will be? Neutral\n",
      "3 Thanks for the great video.\n",
      "\n",
      "I don't understand why the DB continues to be accesible through port 8080 when the local machine connects to the docker container through port 5432? Or, why is it not possible to make GET requests to port 5432 on my local machine?\n",
      "\n",
      "Is it just because the port 5432 is only the port used to connect the app to the database, but the app itself is still handling HTTP through 8080? So an incoming request hits 8080, then goes through 5432 to get to the database? Neutral\n",
      "4 Good person helping good people.\n",
      "This is how it is in America with the exception of NY and DC. Positive\n",
      "5 Dei løk de seim😂 Neutral\n",
      "6 Number two because it looks the best with it Positive\n",
      "7 Thank God we don’t have to listen to his drivel anymore Positive\n",
      "8 Very similar thing happened to me! We lived near a beach that had a stray dog. We would all leave food out for her. One day on my morning walk, I noticed she was near an empty tide pool, empty as in had no water and it was quite steep, when she saw me ran over, she kept looking at the tide pool whining. I went over and noticed a little of puppies. There were seagulls and pelicans around and although the puppies weren’t hers, she knew they weren’t safe. So I stayed with her and the puppies, helping to fend off the sea birds until the mother came back from what I assume was hunting due to the blood on her mouth. That day I ended up adopting two stray dogs and a litter of puppies. One puppy died a few days later but the rest are as healthy as can be!! Positive\n",
      "9 im about to cry😢 Negative\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i,ds2['train'][i]['CommentText'],ds2['train'][i]['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0de90718-6c05-462e-8882-1d9d8eb10b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cooking microwave pizzas, yummy 2 positive\n",
      "1 Any plans of allowing sub tasks to show up in the widget? 1 neutral\n",
      "2  I love the humor, I just reworded it. Like saying 'group therapy' instead`a 'gang banging'. Keeps my moms off my back.   Hahaha 2 positive\n",
      "3  naw idk what ur talkin about 1 neutral\n",
      "4  That sucks to hear. I hate days like that 0 negative\n",
      "5  Umm yeah. That`s probably a pretty good note to self because eeeeeewwwwwwww. 2 positive\n",
      "6  whatever do you mean? 1 neutral\n",
      "7  That would panic me a little!  Maybe you can read on an orbitron at the gym like I do...when all else fails?!? 0 negative\n",
      "8 Is sad when people`s phones are dead 0 negative\n",
      "9   sad face. 0 negative\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i,ds1['train'][i]['text'],ds1['train'][i]['label'],ds1['train'][i]['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f8a1e32-0a66-45a3-8900-15818af46ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds1 = ds1['train']\n",
    "validate_ds1 = ds1['validation']\n",
    "test_ds1 = ds1['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c2a4148-ae27-4da5-84d8-f7f131d06643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e90491bb-1493-4638-babf-7b68c50e0220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www.\\S+\", \"\", text)\n",
    "    text = emoji.demojize(text)  # 😊 → :smiling_face_with_smiling_eyes\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bade0366-7f4b-4631-ac24-9b2a9e37647e",
   "metadata": {},
   "source": [
    "Process Twitter Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c68bccd0-03b4-477a-a933-b4c693939027",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ds1 = [preprocess_text(example['text']) for example in train_ds1]\n",
    "labels_ds1 = [example['label'] for example in train_ds1]  # 0: neg, 1: neutral, 2: pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb2f6adb-4f27-43fa-8595-f06aec88f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_text_ds1 = [preprocess_text(example['text']) for example in validate_ds1]\n",
    "validate_labels_ds1 = [example['label'] for example in validate_ds1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5d000bd-5a13-4f92-8644-b2120cb8c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_ds1 = [preprocess_text(example['text']) for example in test_ds1]\n",
    "test_labels_ds1 = [example['label'] for example in test_ds1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04eee647-01cb-4cad-81ef-5add07bcd6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cooking microwave pizzas, yummy\n"
     ]
    }
   ],
   "source": [
    "print(texts_ds1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952f18bb-eae6-4559-8c78-ee926d2f5d7b",
   "metadata": {},
   "source": [
    "Process Youtube dataset\n",
    "It does not have test and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5a531ca-b5f6-4e24-84e2-cc45e9fad7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ds2 = [preprocess_text(example['CommentText']) for example in ds2['train']]\n",
    "# Map sentiment to numerical labels\n",
    "label_map = {\n",
    "    \"Negative\": 0,\n",
    "    \"Neutral\": 1,\n",
    "    \"Positive\": 2\n",
    "}\n",
    "# Convert labels\n",
    "labels_ds2 = [label_map[example['Sentiment']] for example in ds2['train']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea61d1e1-9c58-45bc-a31d-644f3721dd3f",
   "metadata": {},
   "source": [
    "Tokenization and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6db1b896-6047-4d62-bea6-bcc370253352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e38b0d7-9536-4757-9e4f-3c43db787f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100000\n",
    "max_length = 50\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts_ds1+texts_ds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "734ec099-e8d8-481e-8cf1-a66e0a49249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to sequence of integers\n",
    "sequences_texts_ds1 = tokenizer.texts_to_sequences(texts_ds1)\n",
    "# Pad sequences to equal length\n",
    "padded_texts_ds1 = pad_sequences(sequences_texts_ds1, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "sequences_validate_ds1 = tokenizer.texts_to_sequences(validate_text_ds1)\n",
    "padded_validate_ds1 = pad_sequences(sequences_validate_ds1, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "sequences_test_ds1 = tokenizer.texts_to_sequences(test_text_ds1)\n",
    "padded_test_ds1 = pad_sequences(sequences_test_ds1, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "sequences_ds2 = tokenizer.texts_to_sequences(texts_ds2)\n",
    "padded_ds2 = pad_sequences(sequences_ds2, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c374511f-833c-4342-a6b3-821c807ac25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "sequences_texts_ds2, sequences_validate_ds2, label_train_ds2, label_validate_ds2 = train_test_split(padded_ds2, labels_ds2, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df90269c-752c-4643-b143-dd7ade5a0be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3216, 8943, 20409, 8923]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_texts_ds1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dbecbb-29a4-491a-af59-82c8e5619e21",
   "metadata": {},
   "source": [
    "For ds1:\n",
    "    we have 3 dataset for test, train, and validate\n",
    "    train_ds1 -> texts_ds1(cleaned) -> sequences_texts_ds1(sequence of integers) -> padded_texts_ds1(Equal length)\n",
    "    corresponding labels as 'labels_ds1' for all 3.\n",
    "    similar for other 2 too.\n",
    "\n",
    "For ds2:\n",
    "    we have single dataset train\n",
    "    text_ds2 -> sequences_ds2 -> padded_ds2\n",
    "    corresponding labels are 'labels_ds2'\n",
    "    we divided it into train and validate as [sequences_texts_ds2, label_train_ds2], [sequences_validate_ds2, label_validate_ds2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c34234ae-23d0-4001-a2e1-0e0b5e885a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d2fc162-5a93-4b0a-9b4b-82185267eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f857b93-16f6-4eb9-bfc6-57f286618405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1347fb03-d5f0-407b-a8f8-5defcde15a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_texts_ds1= np.array(padded_texts_ds1)\n",
    "labels_ds1= np.array(labels_ds1)\n",
    "\n",
    "padded_validate_ds1= np.array(padded_validate_ds1)\n",
    "validate_labels_ds1= np.array(validate_labels_ds1)\n",
    "\n",
    "padded_test_ds1= np.array(padded_test_ds1)\n",
    "test_labels_ds1= np.array(test_labels_ds1)\n",
    "\n",
    "sequences_texts_ds2= np.array(sequences_texts_ds2)\n",
    "label_train_ds2= np.array(label_train_ds2)\n",
    "\n",
    "sequences_validate_ds2= np.array(sequences_validate_ds2)\n",
    "label_validate_ds2= np.array(label_validate_ds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70b8f40d-3554-4b6d-9ed2-135a3c12d0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m488/488\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 77ms/step - accuracy: 0.5191 - loss: 0.9419 - val_accuracy: 0.6824 - val_loss: 0.7295\n",
      "Epoch 2/5\n",
      "\u001b[1m488/488\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 74ms/step - accuracy: 0.7481 - loss: 0.6100 - val_accuracy: 0.6788 - val_loss: 0.7256\n",
      "Epoch 3/5\n",
      "\u001b[1m488/488\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 76ms/step - accuracy: 0.8115 - loss: 0.4802 - val_accuracy: 0.6795 - val_loss: 0.7831\n",
      "Epoch 4/5\n",
      "\u001b[1m488/488\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 77ms/step - accuracy: 0.8568 - loss: 0.3830 - val_accuracy: 0.6692 - val_loss: 0.8897\n",
      "Epoch 5/5\n",
      "\u001b[1m488/488\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 78ms/step - accuracy: 0.8851 - loss: 0.3126 - val_accuracy: 0.6596 - val_loss: 0.9818\n"
     ]
    }
   ],
   "source": [
    "# train on ds1 twitter dataset\n",
    "history = model.fit(\n",
    "    padded_texts_ds1, labels_ds1,\n",
    "    epochs=5,\n",
    "    validation_data=(padded_validate_ds1, validate_labels_ds1),\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fdf40618-5087-4806-aa0b-3cd5c6e8e36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m12903/12903\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1128s\u001b[0m 87ms/step - accuracy: 0.6764 - loss: 0.7369 - val_accuracy: 0.7196 - val_loss: 0.6470\n",
      "Epoch 2/5\n",
      "\u001b[1m12903/12903\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1352s\u001b[0m 105ms/step - accuracy: 0.7539 - loss: 0.5786 - val_accuracy: 0.7275 - val_loss: 0.6369\n",
      "Epoch 3/5\n",
      "\u001b[1m12903/12903\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8108s\u001b[0m 628ms/step - accuracy: 0.7956 - loss: 0.4936 - val_accuracy: 0.7247 - val_loss: 0.6539\n",
      "Epoch 4/5\n",
      "\u001b[1m12903/12903\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1533s\u001b[0m 119ms/step - accuracy: 0.8293 - loss: 0.4194 - val_accuracy: 0.7203 - val_loss: 0.7067\n",
      "Epoch 5/5\n",
      "\u001b[1m12903/12903\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1464s\u001b[0m 113ms/step - accuracy: 0.8570 - loss: 0.3543 - val_accuracy: 0.7159 - val_loss: 0.7850\n"
     ]
    }
   ],
   "source": [
    "history_yt = model.fit(\n",
    "    sequences_texts_ds2, label_train_ds2,\n",
    "    epochs=5,\n",
    "    validation_data=(sequences_validate_ds2, label_validate_ds2),\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "896a4030-e48f-461a-be3b-e75c1229ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"sentiment_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53d90f0f-2f13-4898-8e6b-5855d87eee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"sentiment_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf706eb2-f4bd-495a-a0ef-7951d6ec4179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a81e8c76-66c7-4322-8912-89d0f073a06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokenizer.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1e276-782e-4f17-9da0-34241dd8b76a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
